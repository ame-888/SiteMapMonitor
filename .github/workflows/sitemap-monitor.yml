# Simple name for this automated task
name: Sitemap Monitor

# --- When to run this task ---
on:
  schedule:
    # Runs automatically. This example runs every hour at the start of the hour.
    # You can change the schedule - use https://crontab.guru/ to help
    # Be mindful of GitHub's free limits if you make it run very often.
    - cron: '0 * * * *'
  # Allows you to run it manually from the GitHub Actions tab
  workflow_dispatch:

# --- What jobs to run ---
jobs:
  monitor:
    # Use a standard virtual computer provided by GitHub
    runs-on: ubuntu-latest
    steps:
      # Step 1: Get a copy of our repository code (including the Python script we'll add later)
      - name: Checkout code
        uses: actions/checkout@v3

      # Step 2: Set up the Python language environment
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x' # Use a recent version of Python 3

      # Step 3: Install the tools our Python script needs
      - name: Install dependencies
        run: pip install requests beautifulsoup4 lxml # Tools to fetch web pages and read sitemaps

      # Step 4: Run our custom Python script (the brain of the operation)
      - name: Run monitoring script
        id: monitor_script # Give this step an ID so we can refer to its results
        env:
          # <<< --- IMPORTANT: CHANGE THIS URL --- >>>
          SITEMAP_URL: "https://deepmind.google/sitemap.xml" # <<< Paste the ACTUAL sitemap URL you found in Step 2 here!
          KNOWN_URLS_FILE: known_urls.txt # The file where we store the list of pages we already know
          # We will set up the Discord Webhook URL using GitHub Secrets later for security
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL || '' }}
        run: python monitor_script.py # Tell it to run the Python file we'll create next

      # Step 5: Send a Discord notification ONLY IF new pages were found
      - name: Send Discord Notification
        # Condition: Only run if the Python script found new URLs (count > 0) AND a Discord webhook URL is configured
        if: steps.monitor_script.outputs.new_urls_count > 0 && env.DISCORD_WEBHOOK_URL != ''
        run: |
          # Use 'curl' (a command-line tool) to send a message to the Discord webhook
          # The message includes the list of new URLs found by the Python script
          curl -H "Content-Type: application/json" \
               -d '{ "content": "**New URLs detected on ${{ env.SITEMAP_URL }}**:\n```\n${{ steps.monitor_script.outputs.new_urls_list }}\n```" }' \
               ${{ env.DISCORD_WEBHOOK_URL }}

      # Step 6: Save the updated list of known pages back to our repository
      - name: Commit updated known URLs
        # Condition: Only run if the Python script actually found new URLs
        if: steps.monitor_script.outputs.new_urls_count > 0
        run: |
          # Configure Git within the robot's environment
          git config --global user.name 'GitHub Action Bot'
          git config --global user.email 'action@github.com'
          # Add the file containing the list of URLs to Git's tracking
          git add ${{ env.KNOWN_URLS_FILE }}
          # Create a commit message (a record of the change)
          # The '|| echo' prevents errors if the file didn't actually change (shouldn't happen if count > 0, but safe)
          git commit -m "Update known URLs from sitemap" || echo "No changes to commit"
          # Push the changes back to your GitHub repository
          # The '|| echo' prevents errors if the push fails (e.g., simultaneous runs, though unlikely here)
          git push || echo "Push failed (maybe no changes or conflict)"
